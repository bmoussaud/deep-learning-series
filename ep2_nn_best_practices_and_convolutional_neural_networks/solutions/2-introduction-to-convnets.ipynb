{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Yoann/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to convnets\n",
    "\n",
    "This notebook contains the code sample found in Chapter 5, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures.\n",
    "\n",
    "----\n",
    "\n",
    "First, let's take a practical look at a very simple convnet example. We will use our convnet to classify MNIST digits, a task that you've already been through in episode 1, using a densely-connected network (our test accuracy then was 97.8%). Even though our convnet will be very basic, its accuracy will still blow out of the water that of the densely-connected model episode 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Defining the convolutional base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic convnet is a stack of `Conv2D` and `MaxPooling2D` layers. \n",
    "Importantly, a convnet takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension). In our case, we will configure our convnet to process inputs of size `(28, 28, 1)`, which is the format of MNIST images. We do this via passing the argument `input_shape=(28, 28, 1)` to our first layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our convolutional base, using the `Sequential` API.\n",
    "\n",
    "> Here's the network's building blocks : \n",
    "- Conv2D Layer : 32 filters, (3, 3) kernel, relu activation, input shape (28, 28, 1)\n",
    "- MaxPooling2D : pool size (2, 2)\n",
    "- Conv2D Layer : 64 filters, (3, 3) kernel, relu activation\n",
    "- MaxPooling2D : pool size (2, 2)\n",
    "- Conv2D Layer : 64 filters, (3, 3) kernel, relu activation\n",
    "\n",
    "Documentation Conv2D: https://keras.io/layers/convolutional/#conv2d\n",
    "\n",
    "Documentation MaxPooling2D: https://keras.io/layers/pooling/#maxpooling2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 : Create the layers of the convolutional base of the neural network using the Sequential API from Keras\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# YOUR CODE BELOW\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', \n",
    "                        input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the architecture of our convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`. The width and height dimensions tend to shrink as we go deeper in the network. The number of channels is controlled by the first argument passed to the `Conv2D` layers (e.g. 32 or 64)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Feed the convolutional base into a densely-connected classifier network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step would be to feed our last output tensor (of shape `(3, 3, 64)`) into a densely-connected classifier network like those you are already familiar with: a stack of `Dense` layers. These classifiers process vectors, which are 1D, whereas our current output is a 3D tensor. So first, we will have to flatten our 3D outputs to 1D, and then add a few `Dense` layers on top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our densely-connected classifier, using the `Sequential` API.\n",
    "\n",
    "> Here's the network's building blocks : \n",
    "- Flatten layer (no parameters)\n",
    "- Dense Layer : 64 neurons, relu activation\n",
    "- Dense Layer : 10 neurons, softmax activation\n",
    "\n",
    "Documentation Flatten : https://keras.io/layers/core/#flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q2 : Create the last layers of the neural network using the Sequential API from Keras\n",
    "\n",
    "# YOUR CODE BELOW\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do 10-way classification, so we use a final layer with 10 outputs and a softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our `(3, 3, 64)` outputs were flattened into vectors of shape `(576,)`, before going through two `Dense` layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Preprocess the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse a lot of the code we have already covered in the MNIST example from episode 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Compile and fit the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train our convnet on the MNIST digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the following compilation step for your Neural Network : \n",
    "- \"rmsprop\" optimizer\n",
    "- \"categorical_crossentropy\" loss\n",
    "- metric : \"accuracy\"\n",
    "\n",
    "Documentation : https://keras.io/getting-started/sequential-model-guide/#compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.2.1 : Add the compilation step to the network\n",
    "\n",
    "# YOUR CODE BELOW\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train our model for 5 epochs (20 iterations over all samples in the `train_images` and `train_labels` tensors), in mini-batches of 64 samples.\n",
    "\n",
    "Documentation : https://keras.io/models/sequential/#fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 26s 429us/step - loss: 0.1714 - acc: 0.9469\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 25s 417us/step - loss: 0.0474 - acc: 0.9858\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 24s 401us/step - loss: 0.0330 - acc: 0.9899\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 24s 407us/step - loss: 0.0253 - acc: 0.9922\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 25s 410us/step - loss: 0.0203 - acc: 0.9939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11d2e7bd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3.2.2 : Add the compilation step to the network\n",
    "\n",
    "# YOUR CODE BELOW\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the `test_images` and `test_labels` tensors.\n",
    "\n",
    "Documentation : https://keras.io/models/model/#evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 139us/step\n"
     ]
    }
   ],
   "source": [
    "# Q4 : Evaluate the model on the test data\n",
    "\n",
    "# YOUR CODE BELOW\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our densely-connected network from episode 1 had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99.3%: we decreased our error rate by 68% (relative). Not bad! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Observing the errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_images).tolist()\n",
    "true_labels = map(lambda one_hot: np.argmax(one_hot), test_labels.tolist())\n",
    "\n",
    "indexes_errors = []\n",
    "for index, val in enumerate(zip(predictions, true_labels)):\n",
    "    if val[0] != val[1]:\n",
    "        indexes_errors.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12bfa6290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 3\n",
    "\n",
    "plt.imshow(test_images[indexes_errors[index], :, :, 0])\n",
    "plt.title(\"True label : %s / Predicted label : %s\" % (true_labels[indexes_errors[index]], \n",
    "                                                      predictions[indexes_errors[index]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
